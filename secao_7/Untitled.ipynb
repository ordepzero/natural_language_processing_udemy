{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "686ac606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for my NLP Udemy class, which can be found at:\n",
    "# https://deeplearningcourses.com/c/data-science-natural-language-processing-in-python\n",
    "# https://www.udemy.com/data-science-natural-language-processing-in-python\n",
    "# It is written in such a way that tells a story.\n",
    "# i.e. So you can follow a thought process of starting from a\n",
    "# simple idea, hitting an obstacle, overcoming it, etc.\n",
    "# i.e. It is not optimized for anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f644f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: http://lazyprogrammer.me\n",
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a9e7ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4df8262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\pedro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2368cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# from http://www.lextek.com/manuals/onix/stopwords1.html\n",
    "stopwords = set(w.rstrip() for w in open('stopwords.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a7a1df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: an alternative source of stopwords\n",
    "# from nltk.corpus import stopwords\n",
    "# stopwords.words('english')\n",
    "\n",
    "# load the reviews\n",
    "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "positive_reviews = BeautifulSoup(open('electronics/positive.review').read(), features=\"html5lib\")\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('electronics/negative.review').read(), features=\"html5lib\")\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77fb9fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's just try to tokenize the text using nltk's tokenizer\n",
    "# let's take the first review for example:\n",
    "# t = positive_reviews[0]\n",
    "# nltk.tokenize.word_tokenize(t.text)\n",
    "#\n",
    "# notice how it doesn't downcase, so It != it\n",
    "# not only that, but do we really want to include the word \"it\" anyway?\n",
    "# you can imagine it wouldn't be any more common in a positive review than a negative review\n",
    "# so it might only add noise to our model.\n",
    "# so let's create a function that does all this pre-processing for us\n",
    "\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower() # downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # split string into words (tokens)\n",
    "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
    "    tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9e6baf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word_index_map): 10952\n"
     ]
    }
   ],
   "source": [
    "# create a word-to-index map so that we can create our word-frequency vectors later\n",
    "# let's also save the tokenized versions so we don't have to tokenize again later\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "orig_reviews = []\n",
    "\n",
    "for review in positive_reviews:\n",
    "    orig_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "for review in negative_reviews:\n",
    "    orig_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "print(\"len(word_index_map):\", len(word_index_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b615fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create our input matrices\n",
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1) # last element is for the label\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum() # normalize it before setting label\n",
    "    x[-1] = label\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "183e7c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "# (N x D+1 matrix - keeping them together for now so we can shuffle more easily later\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77021cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.781578947368421\n",
      "Test accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# shuffle the data and create train/test splits\n",
    "# try it multiple times!\n",
    "orig_reviews, data = shuffle(orig_reviews, data)\n",
    "\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1]\n",
    "\n",
    "# last 100 rows will be test\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Train accuracy:\", model.score(Xtrain, Ytrain))\n",
    "print(\"Test accuracy:\", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7125858b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this -0.532927637157268\n",
      "unit -0.7498332716090716\n",
      "bad -0.8040212063185823\n",
      "cable 0.7421206076614092\n",
      "time -0.7064927179423838\n",
      "'ve 0.8000522811814239\n",
      "month -0.8505374659136143\n",
      "sound 1.064613260150412\n",
      "lot 0.7857780909956376\n",
      "you 0.9361736814460527\n",
      "n't -2.073351081362845\n",
      "easy 1.7035104525493807\n",
      "quality 1.4105900095452\n",
      "company -0.5753186779366101\n",
      "item -1.0614977416034355\n",
      "wa -1.6204198234581761\n",
      "perfect 1.0516779584753682\n",
      "fast 0.9843212508524014\n",
      "ha 0.630180848522074\n",
      "price 2.66886970645295\n",
      "value 0.5698337645575248\n",
      "money -1.0851910172763397\n",
      "memory 0.9776112628536621\n",
      "picture 0.5096542285180367\n",
      "buy -0.8265665282720345\n",
      "bit 0.5912555459816694\n",
      "happy 0.6027982447868315\n",
      "pretty 0.6256396058851631\n",
      "doe -1.1251967632055244\n",
      "highly 1.0080063856331731\n",
      "customer -0.6566291890769979\n",
      "support -0.8518809221323846\n",
      "little 0.9581204308685742\n",
      "returned -0.7721901920881851\n",
      "excellent 1.4098671993222482\n",
      "love 1.2435024252324849\n",
      "home 0.567844033104217\n",
      "piece -0.5274003134131906\n",
      "useless -0.503073633730249\n",
      "week -0.7223476237314398\n",
      "using 0.6237012230345232\n",
      "laptop 0.5306920473010606\n",
      "video 0.5754287703998245\n",
      "poor -0.73651486228079\n",
      "look 0.5520692452978091\n",
      "then -1.0146310410492332\n",
      "called -0.5076384667956122\n",
      "tried -0.7826158468932478\n",
      "static -0.5021191485592611\n",
      "try -0.6952047179949813\n",
      "space 0.6182042562274463\n",
      "comfortable 0.6629545642999239\n",
      "hour -0.5627619845889543\n",
      "expected 0.6013750892430422\n",
      "speaker 0.8255258944394238\n",
      "warranty -0.6058129193950994\n",
      "stopped -0.5646273407818357\n",
      "junk -0.5465338311200022\n",
      "returning -0.5473191279551941\n",
      "paper 0.6275484086128491\n",
      "terrible -0.5215804897574058\n",
      "return -1.2637474100124688\n",
      "waste -1.0059773456997825\n",
      "refund -0.640974220146547\n"
     ]
    }
   ],
   "source": [
    "# let's look at the weights for each word\n",
    "# try it with different threshold values!\n",
    "threshold = 0.5\n",
    "for word, index in iteritems(word_index_map):\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print(word, weight)\n",
    "\n",
    "\n",
    "# check misclassified examples\n",
    "preds = model.predict(X)\n",
    "P = model.predict_proba(X)[:,1] # p(y = 1 | x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7e20294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most wrong positive review (prob = 0.3393778124352526, pred = 0.0):\n",
      "\n",
      "A device like this either works or it doesn't.  This one happens to work\n",
      "\n",
      "Most wrong negative review (prob = 0.5992399612524896, pred = 1.0):\n",
      "\n",
      "The Voice recorder meets all my expectations and more\n",
      "Easy to use, easy to transfer great results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# since there are many, just print the \"most\" wrong samples\n",
    "minP_whenYis1 = 1\n",
    "maxP_whenYis0 = 0\n",
    "wrong_positive_review = None\n",
    "wrong_negative_review = None\n",
    "wrong_positive_prediction = None\n",
    "wrong_negative_prediction = None\n",
    "for i in range(N):\n",
    "    p = P[i]\n",
    "    y = Y[i]\n",
    "    if y == 1 and p < 0.5:\n",
    "        if p < minP_whenYis1:\n",
    "            wrong_positive_review = orig_reviews[i]\n",
    "            wrong_positive_prediction = preds[i]\n",
    "            minP_whenYis1 = p\n",
    "    elif y == 0 and p > 0.5:\n",
    "        if p > maxP_whenYis0:\n",
    "            wrong_negative_review = orig_reviews[i]\n",
    "            wrong_negative_prediction = preds[i]\n",
    "            maxP_whenYis0 = p\n",
    "\n",
    "print(\"Most wrong positive review (prob = %s, pred = %s):\" % (minP_whenYis1, wrong_positive_prediction))\n",
    "print(wrong_positive_review)\n",
    "print(\"Most wrong negative review (prob = %s, pred = %s):\" % (maxP_whenYis0, wrong_negative_prediction))\n",
    "print(wrong_negative_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e5732",
   "metadata": {},
   "source": [
    "### How to improve\n",
    "##### 1. Try different classifiers\n",
    "##### 2. Try different features\n",
    "##### 3. Try regression\n",
    "##### 4. Try 5 categories instead of 2\n",
    "##### 5. Try different categories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
